{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树模型的各种Ensemble\n",
    "\n",
    "Ensemble的方法主要有两大类：**Bagging**和**Boosting**。\n",
    "\n",
    "## Bagging\n",
    "\n",
    "### 基础Bagging\n",
    "\n",
    "先讲Bagging，Bagging是Bootstrap aggregation的缩写。\n",
    "\n",
    "所谓的Bootstrap是**有放回抽样**，而这里的抽样指的是对数据样本的抽样。\n",
    "\n",
    "如果对于一个有$n$个样本的数据集$D$，有放回抽样$n'$个数据，那么当$n$足够大的时候，满足抽样出来的样本个数（无重复的个数）和原数据集的比例是：$(1 - 1/e) (≈63.2\\%) $\n",
    "\n",
    "**证明**：\n",
    "\n",
    "某个样本没有被抽中的概率是：$p_{not} = (1-\\frac{1}{n})^n$\n",
    "\n",
    "$\\frac{1}{p_{not}} = (\\frac{n}{n-1})^{n} = (1+\\frac{1}{n-1})^{n-1}(1+\\frac{1}{n-1})$\n",
    "\n",
    "当n很大时，上式等于e（根据常用极限：$lim_{x\\rightarrow∞}(1+\\frac{1}{x})^x=e$）。\n",
    "\n",
    "因此，$p_{not} = (1-\\frac{1}{n})^n = \\frac{1}{e}$。 \n",
    "\n",
    "回到Bagging，**Bagging的基本做法**：\n",
    "\n",
    "1 从样本有重复地抽取n个样本\n",
    "\n",
    "2 在所有的属性上，对这n个样本建立分类器\n",
    "\n",
    "3 重复上述过程m次，得到m个分类器\n",
    "\n",
    "4 将数据放在这m个分类器上分类，最终结果由所有分类器结果投票决定\n",
    "\n",
    "**Bagging实践**：\n",
    "\n",
    " `sklearn.ensemble.BaggingClassifier`提供了Bagging的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树测试集正确率75.56%\n",
      "Bagging测试集正确率77.78%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.ensemble import BaggingClassifier  \n",
    "from sklearn import datasets  \n",
    "  \n",
    "#读取数据，划分训练集和测试集  \n",
    "iris=datasets.load_iris()  \n",
    "x=iris.data[: , :2]  \n",
    "y=iris.target  \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=1)  \n",
    "  \n",
    "#模型训练  \n",
    "# sklearn 自带的决策树分类器\n",
    "model1=DecisionTreeClassifier(max_depth=3)  \n",
    "# sklearn自带的bagging分类器\n",
    "model2=BaggingClassifier(model1,n_estimators=100,max_samples=0.3)  \n",
    "model1.fit(x_train,y_train)  \n",
    "model2.fit(x_train,y_train)  \n",
    "model1_pre=model1.predict(x_test)  \n",
    "model2_pre=model2.predict(x_test)  \n",
    "res1=model1_pre==y_test  \n",
    "res2=model2_pre==y_test  \n",
    "print '决策树测试集正确率%.2f%%'%np.mean(res1*100)  \n",
    "print 'Bagging测试集正确率%.2f%%'%np.mean(res2*100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林（Random Forest）\n",
    "\n",
    "Bagging是在样本上做抽样，如果将抽样思想用在特征上，即**对特征进行无放回抽样**，并且使用CART树，这就是**随机森林**。\n",
    "\n",
    "**随机森林的基本做法**：\n",
    "\n",
    "1 首先在样本集中有放回的抽样n个样本\n",
    "\n",
    "2 在所有的属性当中再随机选择K个属性\n",
    "\n",
    "3 根据这n个样本的K个属性，建立CART树\n",
    "\n",
    "4 重复以上过程m次，得到了m棵CART树\n",
    "\n",
    "5 利用这m棵树对样本进行预测并投票\n",
    "\n",
    "**随机森林实践**：\n",
    "\n",
    " `sklearn.ensemble.RandomForestClassifier`提供了随机森林的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树训练集正确率83.81%\n",
      "随机森林训练集正确率85.71%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn import datasets  \n",
    "  \n",
    "#读取数据，划分训练集和测试集  \n",
    "iris=datasets.load_iris()  \n",
    "x=iris.data[:,:2]  \n",
    "y=iris.target  \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=1)  \n",
    "  \n",
    "#模型训练  \n",
    "model1=DecisionTreeClassifier(max_depth=3)  \n",
    "model2=RandomForestClassifier(n_estimators=200, criterion='entropy', max_depth=3)  \n",
    "model1.fit(x_train,y_train)  \n",
    "model2.fit(x_train,y_train)  \n",
    "model1_pre=model1.predict(x_train)  \n",
    "model2_pre=model2.predict(x_train)  \n",
    "res1=model1_pre==y_train  \n",
    "res2=model2_pre==y_train  \n",
    "print '决策树训练集正确率%.2f%%'%np.mean(res1*100)  \n",
    "print '随机森林训练集正确率%.2f%%'%np.mean(res2*100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting（提升）通过**给样本设置不同的权值**，每轮迭代调整权值。\n",
    "\n",
    "不同的提升算法之间的差别，一般是：\n",
    "\n",
    "（1）如何更新**样本的权值**；\n",
    "\n",
    "（2）如何组合每个分类器的预测，即，调整**分类器的权值**。\n",
    "\n",
    "其中Adaboost中，样本权值是增加那些被错误分类的样本的权值，分类器$C_i$的重要性依赖于它的错误率。\n",
    "\n",
    "Boosting主要关注**降低偏差**，因此Boost能基于泛化性能相当弱的学习器构建出很强的集成；\n",
    "\n",
    "Bagging主要关注**降低方差**，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。\n",
    "\n",
    "\n",
    "### GBDT\n",
    "\n",
    "GBDT（Gradient Boosting Decision Tree）又叫 MART（Multiple Additive Regression Tree)，从后面的名字可以看出，这是一种**回归树**的模型，即，其用于预测实数值，target的加减运算，是有意义的。相反，分类的加减运算没有意义。\n",
    "\n",
    "\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
